#RAG-based Document Question Answering with Ollama + Flask

This project is a **Retrieval-Augmented Generation (RAG) web application** built using **Flask, LangChain, and ChromaDB**.  
It enables users to upload PDF documents, process them into embeddings, and query them with the help of **Large Language Models (LLMs)** served via **Ollama**.  

---

Features
- **PDF ingestion** â€“ Upload and process PDF documents into searchable chunks.  
- **Vector embeddings** â€“ Store and retrieve context using **ChromaDB**.  
- **LLM-powered Q&A** â€“ Ask questions and get context-aware answers using **Ollama LLMs**.  
- **Web interface** â€“ Simple Flask-based frontend for interaction.  
- **Public access via Ngrok** â€“ Share your running app with others.  

---

## ðŸ›  Installation

### 1. Clone the repository
```bash
git clone <your-repo-link>
cd rag-project
